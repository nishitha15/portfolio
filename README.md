# Data Engineer

**#About Me**

I'm a Data Engineer with 6 years of experience building and optimizing ETL pipelines, data architectures, and big data solutions across AWS, Azure, and GCP. I have hands-on expertise in Apache Spark (PySpark, Scala), Hadoop, and Kafka, handling large-scale data processing efficiently. I specialize in data modeling, warehousing, and performance tuning using Snowflake, Redshift, and Synapse Analytics. I’ve worked extensively with Apache Airflow, Databricks, and cloud-native data services to streamline workflows and enable real-time analytics. My core skills include SQL, Python, and Scala, with a strong focus on data transformation and automation. I also have experience with CI/CD pipelines, DevOps practices, and containerization (Docker, Kubernetes). I enjoy collaborating with teams to build scalable, cost-effective data solutions and leverage cloud and big data technologies to power business intelligence and analytics.

**Technical Expertise**
•	Programming: Python, SQL, Bash, Shell scripting
•	Frameworks: Databricks, Snowflake, Airflow
•	Database: PostgreSQL, Amazon Redshift, Oracle, MySQL, MongoDB	 
•	API: RESTful API Design, API Gateway, MuleSoft API.
•	Data Integration: AWS Glue, Apache NiFi, Informatica, Azure Data Factory, Cloud Data Fusion	 	•	⁠Cloud Platforms: AWS (S3, Redshift, Glue, Lambda, Athena, EMR, Kinesis), Azure (ADF, Synapse, Data Lake), GCP (BigQuery, Dataflow, Cloud Composer, Pub/Sub)	 
•	⁠Data Modeling: Normalization, Star Schema, Snowflake Schema
•	⁠Tools: Git, Jenkins, Docker, Kubernetes, Jira, 
•	⁠Testing: Unit Testing, Integration Testing, Performance Tuning

# Professional Experience


	 
